<!DOCTYPE HTML>
<!--
Massively by HTML5 UP
html5up.net | @ajlkn
Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
    <head>
        <title>SAIR Publications </title>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
        <link rel="stylesheet" href="assets/css/main.css" />
        <noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
    </head>
    <body class="is-preload">
        
        <!-- Wrapper -->
        <div id="wrapper">
            
            <!-- Header -->
            <header id="header">
                <a href="index.html" class="logo">SAIR</a>
            </header>
            
            <!-- Nav -->
            <nav id="nav">
                <ul class="links">
                    <li><a href="index.html">About</a></li>
                    <li><a href="news.html">news</a></li>
                    <li class="active"><a href="publications.html">Publications</a></li>
                    <li><a href="projects.html">Projects</a></li>
                    <li><a href="collaborators.html">Collaborators</a></li>
                    <li><a href="events.html">Events</a></li>
                </ul>
                <ul class="icons">
                    <li><a href="https://twitter.com/SAIRLab" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
                    <li><a href="https://www.youtube.com/channel/UClbQ8YDyYHe2UxW5448_LJw" class="icon brands fa-youtube"><span class="label">YouTube</span></a></li>
                    <li><a href="https://github.com/sairlab" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
                </ul>
            </nav>
            
            <!-- Main -->
            <div id="main">
                
                <section id="publications">
                <table>

                        <tr>
                            <td><a href=""><img style="vertical-align:middle" src="images/paperpreviews/agreeor.png"  width="200" height="inherit" border="1px" alt="" /></a></td>
                            <th>
                                <b>Agree or Disagree? Generating Body Gestures from Affective Contextual Cues during Dyadic Interactions.</b> <br>
                                <b>Nguyen Tan Viet Tuyen, and Oya Celiktutan<br>
                                <em> 31st IEEE International Conference on Robot and Human Interactive Communication (RO-MAN'22) </em> <br>
                                [<a href="https://ieeexplore.ieee.org/document/9900760/">Paper</a>] <!-- [<a href="projects/Robocentric_CGD_ROMAN2020/UKRAS20_paper_14.pdf">Paper</a>] projects/ReinforcementLearning/index.html -->
                            </th>
                        </tr>
                        <tr>
                            <td><a href=""><img style="vertical-align:middle" src="images/paperpreviews/analysingeyegaze.png"  width="200" height="inherit" border="1px" alt="" /></a></td>
                            <th>
                                <b>Analysing eye gaze patterns during confusion and errors in human–agent collaborations</b> <br>
                                <b>Lennart Wachowiak, Peter Tisnikar, Gerard Canal, Andrew Coles, Matteo Leonetti, and Oya Celiktutan<br>
                                <em> 31st IEEE International Conference on Robot and Human Interactive Communication (RO-MAN'22) </em> <br>
                                [<a href="https://kclpure.kcl.ac.uk/portal/files/175257978/Human_Eye_Gaze_final.pdf">Paper</a>] <!-- [<a href="projects/Robocentric_CGD_ROMAN2020/UKRAS20_paper_14.pdf">Paper</a>] projects/ReinforcementLearning/index.html -->
                            </th>
                        </tr>
                        <tr>
                            <td><a href=""><img style="vertical-align:middle" src="images/paperpreviews/automaticcontext.png"  width="200" height="inherit" border="1px" alt="" /></a></td>
                            <th>
                                <b>Automatic Context-Driven Inference of Engagement in HMI: A Survey</b> <br>
                                <b>Hanan Salam, Oya Celiktutan, Hatice Gunes, and Mohamed Chetouani<br>
                                <em>  </em> <br>
                                [<a href="https://arxiv.org/pdf/2209.15370">Paper</a>] <!-- [<a href="projects/Robocentric_CGD_ROMAN2020/UKRAS20_paper_14.pdf">Paper</a>] projects/ReinforcementLearning/index.html -->
                            </th>
                        </tr>
                        <tr>
                            <td><a href=""><img style="vertical-align:middle" src="images/paperpreviews/personalized.png"  width="200" height="inherit" border="1px" alt="" /></a></td>
                            <th>
                                <b>Personalized Productive Engagement Recognition in Robot-Mediated Collaborative Learning</b> <br>
                                <b>Vetha Vikashini, Hanan Salam, Jauwairia Nasir, Barbara Bruno, and Oya Celiktutan<br>
                                <em> 24th ACM International Conference on Multimodal Interaction (ICMI'22) </em> <br>
                                [<a href="https://scholar.google.co.uk/citations?view_op=view_citation&hl=en&user=CCCoMqcAAAAJ&sortby=pubdate&authuser=1&citation_for_view=CCCoMqcAAAAJ:yD5IFk8b50cC">Paper</a>] <!-- [<a href="projects/Robocentric_CGD_ROMAN2020/UKRAS20_paper_14.pdf">Paper</a>] projects/ReinforcementLearning/index.html -->
                            </th>
                        </tr>
                        <tr>
                            <td><a href=""><img style="vertical-align:middle" src="images/paperpreviews/growl.png"  width="200" height="inherit" border="1px" alt="" /></a></td>
                            <th>
                                <b>GROWL: Group Detection With Link Prediction</b> <br>
                                <b>Viktor Schmuck, and Oya Celiktutan<br>
                                <em> 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG'21) </em> <br>
                                [<a href="https://ieeexplore.ieee.org/document/9667061">Paper</a>] <!-- [<a href="projects/Robocentric_CGD_ROMAN2020/UKRAS20_paper_14.pdf">Paper</a>] projects/ReinforcementLearning/index.html -->
                            </th>
                        </tr>
                        <tr>
                            <td><a href=""><img style="vertical-align:middle" src="projects/ReinforcementLearning/thumb.png"  width="200" height="inherit" border="1px" alt="" /></a></td>
                            <th>
                                <b>Learning Routines for Effective Off-policy Reinforcement Learning</b> <br>
                                <b> Edoardo Cetin, and Oya Celiktutan<br>
                                <em> International Conference on Machine Learning 2021 (ICML'21) </em> <br>
                                [<a href="https://sites.google.com/view/routines-rl/home">Project page</a>] <!-- [<a href="projects/Robocentric_CGD_ROMAN2020/UKRAS20_paper_14.pdf">Paper</a>] projects/ReinforcementLearning/index.html -->
                            </th>
                        </tr>

                        <tr>
                            <th><a href="projects/ContinualLearning/index.html"><img style="vertical-align:middle" src="projects/ContinualLearning/thumb.png"  width="200" height="inherit" border="1px" alt="" /></a></td>
                            <th>
                                <b>IB-DRR: Incremental Learning with Information-Back Discrete Representation Replay</b> <br>
                                <b> Jian Jiang, Edoardo Cetin, and Oya Celiktutan<br>
                                <em>CVPR Workshop on Continual Learning 2021 (CLVision'21) </em> <br>
                                [<a href="https://arxiv.org/pdf/2104.10588v1.pdf">Paper</a>] <!-- [<a href="projects/Robocentric_CGD_ROMAN2020/UKRAS20_paper_14.pdf">Paper</a>] -->
                            </th>
                        </tr>
                        
                        <tr>
                            <td><a href="projects/ObservationalImitation/index.html"><img style="vertical-align:middle" src="projects/ObservationalImitation/thumb.png"  width="200" height="inherit" border="1px" alt="" /></a></td>
                            <th>
                                <b>Domain-Robust Visual Imitation Learning with Mutual Information Constraints </b> <br>
                                <b> Edoardo Cetin, and Oya Celiktutan<br>
                                <em>International Conference on Learning Representations 2021 (ICLR'21) </em> <br>
                                [<a href="https://sites.google.com/view/disentangail/">Project page</a>] <!-- [<a href="projects/Robocentric_CGD_ROMAN2020/UKRAS20_paper_14.pdf">Paper</a>] -->
                            </th>
                        </tr>
                        
                        <tr>
                            <td><a href="projects/Robocentric_CGD_ROMAN2020/index.html"><img style="vertical-align:middle" src="projects/Robocentric_CGD_ROMAN2020/thumb.jpg"  width="200" height="inherit" border="1px" alt="" /></a></td>
                            <th>
                                <b>Robocentric Conversational Group Discovery</b> <br>
                                <b>Viktor Schmuck, Tingran Sheng, and Oya Celiktutan<br>
                                <em> IEEE International Conference on Robot & Human Interactive Communication 2020 (RO-MAN'20) </em> <br>
                                [<a href="projects/Robocentric_CGD_ROMAN2020/index.html">Project page</a>] <!-- [<a href="projects/Robocentric_CGD_ROMAN2020/UKRAS20_paper_14.pdf">Paper</a>] -->
                            </th>
                        </tr>
                        
                        <tr>
                            <td><a href="projects/RICA_UKRAS/index.html"><img style="vertical-align:middle" src="projects/RICA_UKRAS/thumb.jpg"  width="200" height="inherit" border="1px" alt="" /></a></td>
                            <th>
                                <b>RICA: Robocentric Indoor Crowd Analysis Dataset</b> <br>
                                <b>Viktor Schmuck, and Oya Celiktutan<br>
                                <em>UKRAS20 Conference: “Robots into the real world” Proceedings, 2020</em> <br>
                                [<a href="projects/RICA_UKRAS/index.html">Project page</a>] <!--  [<a href="projects/RICA_UKRAS/UKRAS20_paper_14.pdf">Paper</a>]-->
                            </th>
                        </tr>
                        <tr>
                            <td><a href="projects/IGTD2020/index.html"><img style="vertical-align:middle" src="projects/IGTD2020/teaser.png"  width="200" height="inherit" border="1px" alt="" /></a></td>
                            <th>
                                <b>Inferring Student Engagement in Collaborative Problem Solving from Visual Cues</b> <br>
                                <b>Angelika Kasparova, Oya Celiktutan, and Mutlu Cukurova<br>
                                <em>International Conference on Multimodal Interaction Workshops 2020 (ICMI'20 Companion)</em> <br>
                                [<a href="projects/IGTD2020/index.html">Project page</a>]
                            </th>
                        </tr>
                    </table>
                </section>

                
            </div>

            <!-- Footer -->
            <footer id="footer">
                
                <section class="split contact">
                    <section class="alt">
                        <h3>Address</h3>
                        <p> King’s College London, Faculty of Natural and Mathematical Sciences, Department of Engineering, Strand, London WC2R 2LS UK</p>
                    </section>
                    <section>
                        <h3>Email</h3>
                        <p><a href="mailto:oya.celiktutan@kcl.ac.uk">Contact Oya</a></p>
                    </section>
                    <section>
                        <h3>Social</h3>
                        <ul class="icons alt">
                            <li><a href="https://twitter.com/SAIRLab" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
                            <li><a href="https://www.youtube.com/channel/UClbQ8YDyYHe2UxW5448_LJw" class="icon brands fa-youtube"><span class="label">YouTube</span></a></li>
                            <li><a href="https://github.com/sairlab" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
                        </ul>
                    </section>
                </section>
            </footer>
            
            <!-- Copyright -->
            <div id="copyright">
                <ul><li>&copy; King's College London </li><li>We thank <a href="https://html5up.net">HTML5 UP</a> for providing this template. </li></ul>
            </div>
            
        </div>
        <!-- Scripts -->
        <script src="assets/js/jquery.min.js"></script>
        <script src="assets/js/jquery.scrollex.min.js"></script>
        <script src="assets/js/jquery.scrolly.min.js"></script>
        <script src="assets/js/browser.min.js"></script>
        <script src="assets/js/breakpoints.min.js"></script>
        <script src="assets/js/util.js"></script>
        <script src="assets/js/main.js"></script>
        
    </body>
</html>
