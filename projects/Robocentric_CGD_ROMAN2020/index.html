---
layout: default1
title: "Robocentric Conversational Group Discovery"
---

<center><h1>{{ page.title }}</h1></center>

<td>
	<center>
		Accepted to ROMAN2020: The 29th IEEE International Conference on Robot & Human Interactive Communication Proceedings<br>
		<br>
		<nobr>Viktor Schmuck </nobr> &emsp;&emsp; Tingran Sheng </nobr> &emsp;&emsp; <nobr>Oya Celiktutan </nobr><br>
		<br>
		<nobr>Department of Engineering, King's College London</nobr><br>
		<br>
		<img style="vertical-align:middle" src="teaser.png"  width="80%" height="inherit"/>		
	</center>
</td>

<br>
	
<td>
	<hr>
	<h3 style="margin-bottom:10px;">Abstract</h3>
	Detection of people interacting and conversing with each other is essential to equip social robots with autonomous navigation and service capabilities in crowded social scenes. In this paper, we introduced a method for unsupervised conversational group detection in images captured from a mobile robot's perspective. To this end, we collected a novel dataset called Robocentric Indoor Crowd Analysis (RICA). The RICA dataset features over 100,000 RGB, depth, and wide-angle camera images as well as LIDAR readings, recorded during a social event where the robot navigated between the participants and captured interactions among groups using its on-board sensors. Using the RICA dataset, we implemented an unsupervised group detection method based on agglomerative hierarchical clustering. Our results show that incorporating the depth modality and using normalised features in the clustering algorithm improved the group detection accuracy by a margin of 3% on average.
</td>

<td>
	<h3> Paper: [<a href="https://kclpure.kcl.ac.uk/portal/en/publications/robocentric-conversational-group-discovery(13872a46-7b4f-4028-8d51-5f21ca3fec43).html">PDF</a>] <!-- [<a href="UKRAS20_paper_14.pdf">PDF</a>] --> &nbsp; &nbsp; &nbsp; Proceedings: [Link Coming soon]  &nbsp; &nbsp; &nbsp; Data: [Contact <a href="mailto:sairlteam@gmail.com">SAIR Lab</a>]<!-- [<a href="https://www.ukras.org/publications/ras-proceedings/UKRAS20/pp63-65">UKRAS20</a>] --> </h3>
</td>
<!-- <td>
	<h3> Paper: [<a href="UKRAS20_paper_14.pdf">PDF</a>] &nbsp; &nbsp; &nbsp; Code: [<a href="https://github.com/google-research/motion_imitation">GitHub</a>] &nbsp; &nbsp; &nbsp; Blog: [<a href="https://ai.googleblog.com/2020/04/exploring-nature-inspired-robot-agility.html">Google</a> / <a href="https://bair.berkeley.edu/blog/2020/04/03/laikago/">BAIR</a>] &nbsp; &nbsp; &nbsp; Preprint: [<a href="http://arxiv.org/abs/2004.00784">arXiv</a>] </h3>
</td> -->

<!-- <tr>
		<h3 style="margin-bottom:10px;">Video</h3>
		<iframe width="560" height="315" src="https://www.youtube.com/embed/lKYh6uuCwRY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</tr> -->
	
<br>
<br>

<h3 style="margin-bottom:0px;">Bibtex</h3>
<pre> 
@inproceedings{ViktorEtAlROMAN20,
author = {Viktor Schmuck and Tingran Sheng and Oya Celiktutan},
booktitle = {The 29th IEEE International Conference on Robot & Human Interactive Communication (RO-MAN)},
title = {Robocentric Conversational Group Discovery}, 
year = {2020} } 
</pre>
